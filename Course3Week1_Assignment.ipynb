{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOgZ3X6C7oZFQTqSbOGWW9E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yohanes213/TensorFlow_Coursera/blob/main/Course3Week1_Assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiI6a3rV3pZp"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "# # Week 1: Explore the BBC News archive\n",
        "# \n",
        "# Welcome! In this assignment you will be working with a variation of the [BBC News Classification Dataset](https://www.kaggle.com/c/learn-ai-bbc/overview), which contains 2225 examples of news articles with their respective categories (labels).\n",
        "# \n",
        "# Let's get started!\n",
        "\n",
        "# In[1]:\n",
        "\n",
        "\n",
        "import csv\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "# Begin by looking at the structure of the csv that contains the data:\n",
        "\n",
        "# In[2]:\n",
        "\n",
        "\n",
        "with open(\"./data/bbc-text.csv\", 'r') as csvfile:\n",
        "    print(f\"First line (header) looks like this:\\n\\n{csvfile.readline()}\")\n",
        "    print(f\"Each data point looks like this:\\n\\n{csvfile.readline()}\")     \n",
        "\n",
        "\n",
        "# As you can see, each data point is composed of the category of the news article followed by a comma and then the actual text of the article.\n",
        "\n",
        "# ## Removing Stopwords\n",
        "# \n",
        "# One important step when working with text data is to remove the **stopwords** from it. These are the most common words in the language and they rarely provide useful information for the classification process.\n",
        "# \n",
        "# Complete the `remove_stopwords` below. This function should receive a string and return another string that excludes all of the stopwords provided.\n",
        "\n",
        "# In[59]:\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: remove_stopwords\n",
        "def remove_stopwords(sentence):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "    \n",
        "    Args:\n",
        "        sentence (string): sentence to remove the stopwords from\n",
        "    \n",
        "    Returns:\n",
        "        sentence (string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "    stopwords = [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    \n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    \n",
        "    without_stop = [word for word in sentence.split(' ') if not word in stopwords]\n",
        "    sentence=\"\"\n",
        "    for i in without_stop:\n",
        "        sentence+=i\n",
        "        if i!=without_stop[-1]:\n",
        "            sentence+=' '    \n",
        "    ### END CODE HERE\n",
        "    return sentence\n",
        "\n",
        "\n",
        "# In[60]:\n",
        "\n",
        "\n",
        "# Test your function\n",
        "remove_stopwords(\"I am about to go to the store and get any snack\")\n",
        "\n",
        "\n",
        "# ***Expected Output:***\n",
        "# ```\n",
        "# 'go store get snack'\n",
        "# \n",
        "# ```\n",
        "\n",
        "# ## Reading the raw data\n",
        "# \n",
        "# Now you need to read the data from the csv file. To do so, complete the `parse_data_from_file` function.\n",
        "# \n",
        "# A couple of things to note:\n",
        "# - You should omit the first line as it contains the headers and not data points.\n",
        "# - There is no need to save the data points as numpy arrays, regular lists is fine.\n",
        "# - To read from csv files use [`csv.reader`](https://docs.python.org/3/library/csv.html#csv.reader) by passing the appropriate arguments.\n",
        "# - `csv.reader` returns an iterable that returns each row in every iteration. So the label can be accessed via row[0] and the text via row[1].\n",
        "# - Use the `remove_stopwords` function in each sentence.\n",
        "\n",
        "# In[61]:\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: parse_data_from_file\n",
        "def parse_data_from_file(filename):\n",
        "    \"\"\"\n",
        "    Extracts sentences and labels from a CSV file\n",
        "    \n",
        "    Args:\n",
        "        filename (string): path to the CSV file\n",
        "    \n",
        "    Returns:\n",
        "        sentences, labels (list of string, list of string): tuple containing lists of sentences and labels\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    labels = []\n",
        "    with open(filename, 'r') as csvfile:\n",
        "        ### START CODE HERE\n",
        "        reader = csv.reader(csvfile, delimiter=',')\n",
        "        next(reader)\n",
        "        for item in reader:\n",
        "            sentences.append(remove_stopwords(item[1]))\n",
        "            labels.append(item[0])\n",
        "        ### END CODE HERE\n",
        "    return sentences, labels\n",
        "\n",
        "\n",
        "# In[62]:\n",
        "\n",
        "\n",
        "# Test your function\n",
        "\n",
        "# With original dataset\n",
        "sentences, labels = parse_data_from_file(\"./data/bbc-text.csv\")\n",
        "\n",
        "print(\"ORIGINAL DATASET:\\n\")\n",
        "print(f\"There are {len(sentences)} sentences in the dataset.\\n\")\n",
        "print(f\"First sentence has {len(sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(labels)} labels in the dataset.\\n\")\n",
        "print(f\"The first 5 labels are {labels[:5]}\\n\\n\")\n",
        "\n",
        "# With a miniature version of the dataset that contains only first 5 rows\n",
        "mini_sentences, mini_labels = parse_data_from_file(\"./data/bbc-text-minimal.csv\")\n",
        "\n",
        "print(\"MINIATURE DATASET:\\n\")\n",
        "print(f\"There are {len(mini_sentences)} sentences in the miniature dataset.\\n\")\n",
        "print(f\"First sentence has {len(mini_sentences[0].split())} words (after removing stopwords).\\n\")\n",
        "print(f\"There are {len(mini_labels)} labels in the miniature dataset.\\n\")\n",
        "print(f\"The first 5 labels are {mini_labels[:5]}\")\n",
        "\n",
        "\n",
        "# ***Expected Output:***\n",
        "# ```\n",
        "# ORIGINAL DATASET:\n",
        "# \n",
        "# There are 2225 sentences in the dataset.\n",
        "# \n",
        "# First sentence has 436 words (after removing stopwords).\n",
        "# \n",
        "# There are 2225 labels in the dataset.\n",
        "# \n",
        "# The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n",
        "# \n",
        "# \n",
        "# MINIATURE DATASET:\n",
        "# \n",
        "# There are 5 sentences in the miniature dataset.\n",
        "# \n",
        "# First sentence has 436 words (after removing stopwords).\n",
        "# \n",
        "# There are 5 labels in the miniature dataset.\n",
        "# \n",
        "# The first 5 labels are ['tech', 'business', 'sport', 'sport', 'entertainment']\n",
        "# \n",
        "# ```\n",
        "\n",
        "# ## Using the Tokenizer\n",
        "# \n",
        "# Now it is time to tokenize the sentences of the dataset. \n",
        "# \n",
        "# Complete the `fit_tokenizer` below. \n",
        "# \n",
        "# This function should receive the list of sentences as input and return a [Tokenizer](https://www.tensorflow.org/api_docs/python/tf/keras/preprocessing/text/Tokenizer) that has been fitted to those sentences. You should also define the \"Out of Vocabulary\" token as `<OOV>`.\n",
        "\n",
        "# In[65]:\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: fit_tokenizer\n",
        "def fit_tokenizer(sentences):\n",
        "    \"\"\"\n",
        "    Instantiates the Tokenizer class\n",
        "    \n",
        "    Args:\n",
        "        sentences (list): lower-cased sentences without stopwords\n",
        "    \n",
        "    Returns:\n",
        "        tokenizer (object): an instance of the Tokenizer class containing the word-index dictionary\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    # Instantiate the Tokenizer class by passing in the oov_token argument\n",
        "    tokenizer = Tokenizer(oov_token= \"<oov>\")\n",
        "    # Fit on the sentences\n",
        "    tokenizer.fit_on_texts(sentences)\n",
        "    ### END CODE HERE\n",
        "    return tokenizer\n",
        "\n",
        "\n",
        "# In[66]:\n",
        "\n",
        "\n",
        "tokenizer = fit_tokenizer(sentences)\n",
        "word_index = tokenizer.word_index\n",
        "\n",
        "print(f\"Vocabulary contains {len(word_index)} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n",
        "\n",
        "\n",
        "# ***Expected Output:***\n",
        "# ```\n",
        "# Vocabulary contains 29714 words\n",
        "# \n",
        "# <OOV> token included in vocabulary\n",
        "# \n",
        "# ```\n",
        "\n",
        "# In[69]:\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: get_padded_sequences\n",
        "def get_padded_sequences(tokenizer, sentences):\n",
        "    \"\"\"\n",
        "    Generates an array of token sequences and pads them to the same length\n",
        "    \n",
        "    Args:\n",
        "        tokenizer (object): Tokenizer instance containing the word-index dictionary\n",
        "        sentences (list of string): list of sentences to tokenize and pad\n",
        "    \n",
        "    Returns:\n",
        "        padded_sequences (array of int): tokenized sentences padded to the same length\n",
        "    \"\"\"\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the post padding strategy\n",
        "    padded_sequences = pad_sequences(sequences , padding= 'post')\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return padded_sequences\n",
        "\n",
        "\n",
        "# In[70]:\n",
        "\n",
        "\n",
        "padded_sequences = get_padded_sequences(tokenizer, sentences)\n",
        "print(f\"First padded sequence looks like this: \\n\\n{padded_sequences[0]}\\n\")\n",
        "print(f\"Numpy array of all sequences has shape: {padded_sequences.shape}\\n\")\n",
        "print(f\"This means there are {padded_sequences.shape[0]} sequences in total and each one has a size of {padded_sequences.shape[1]}\")\n",
        "\n",
        "\n",
        "# ***Expected Output:***\n",
        "# ```\n",
        "# First padded sequence looks like this: \n",
        "# \n",
        "# [  96  176 1157 ...    0    0    0]\n",
        "# \n",
        "# Numpy array of all sequences has shape: (2225, 2438)\n",
        "# \n",
        "# This means there are 2225 sequences in total and each one has a size of 2438\n",
        "# \n",
        "# ```\n",
        "\n",
        "# In[71]:\n",
        "\n",
        "\n",
        "# GRADED FUNCTION: tokenize_labels\n",
        "def tokenize_labels(labels):\n",
        "    \"\"\"\n",
        "    Tokenizes the labels\n",
        "    \n",
        "    Args:\n",
        "        labels (list of string): labels to tokenize\n",
        "    \n",
        "    Returns:\n",
        "        label_sequences, label_word_index (list of string, dictionary): tokenized labels and the word-index\n",
        "    \"\"\"\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class\n",
        "    # No need to pass additional arguments since you will be tokenizing the labels\n",
        "    label_tokenizer = Tokenizer()\n",
        "    \n",
        "    # Fit the tokenizer to the labels\n",
        "    label_tokenizer.fit_on_texts(labels)\n",
        "    \n",
        "    # Save the word index\n",
        "    label_word_index = label_tokenizer.word_index\n",
        "    \n",
        "    # Save the sequences\n",
        "    label_sequences = label_tokenizer.texts_to_sequences(labels)\n",
        "\n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return label_sequences, label_word_index\n",
        "\n",
        "\n",
        "# In[72]:\n",
        "\n",
        "\n",
        "label_sequences, label_word_index = tokenize_labels(labels)\n",
        "print(f\"Vocabulary of labels looks like this {label_word_index}\\n\")\n",
        "print(f\"First ten sequences {label_sequences[:10]}\\n\")\n",
        "\n",
        "\n",
        "# ***Expected Output:***\n",
        "# ```\n",
        "# Vocabulary of labels looks like this {'sport': 1, 'business': 2, 'politics': 3, 'tech': 4, 'entertainment': 5}\n",
        "# \n",
        "# First ten sequences [[4], [2], [1], [1], [5], [3], [3], [1], [1], [5]]\n",
        "# \n",
        "# ```\n",
        "\n",
        "# **Congratulations on finishing this week's assignment!**\n",
        "# \n",
        "# You have successfully implemented functions to process various text data processing ranging from pre-processing, reading from raw files and tokenizing text.\n",
        "# \n",
        "# **Keep it up!**"
      ]
    }
  ]
}